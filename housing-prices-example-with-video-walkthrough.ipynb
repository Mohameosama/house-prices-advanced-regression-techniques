{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5407,"databundleVersionId":868283,"sourceType":"competition"}],"dockerImageVersionId":30458,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Intro Housing Price Regression Walkthrough \nIn this workbook, I walk through an analysis of the Housing Price Dataset. There is also an accompanying video on YouTube located here: https://youtu.be/NQQ3DRdXAXE\n\nI touch on a few things in the **notebook**:\n1. Basic data cleaning and feature exploration\n2. Exploratory data analysis (Answering questions we have of the data)\n3. Basic Data Engineering (Creating a pipeline for tain and test sets)\n4. Model Experimentation and parameter tuning (Linear Regression, Random Forest, XGBoost, MLP)\n5. Feature Engineering \n6. Ensembling \n7. Submitting to the Competition\n\nThings I touch on in the **video**:\n1. How to approach a problem like this \n2. How I would consider using AI tools like ChatGPT to solve a problem like this \n3. Why I made certain design decisions and the choices we have we we do open ended projects like these\n4. How you can continue and improve upon this analysis \n\nI have done something similar in the past with the **Titanic Dataset** if you want something slighty more beginner friendly:\n\n- Kaggle notebook: https://www.kaggle.com/code/kenjee/titanic-project-example/notebook\n- YouTube Video: https://www.youtube.com/watch?v=I3FBJdiExcg&ab_channel=KenJee\n\nMy github repos with additional free and paid resources: \n- ML Process: https://github.com/PlayingNumbers/ML_Process_Course\n- ML ALgorithms: https://github.com/PlayingNumbers/ML_Algorithms_Course","metadata":{}},{"cell_type":"code","source":"#import relevant packages \nimport pandas as pd \nimport numpy as np \nimport plotly.graph_objects as go\nimport plotly.express as px\nimport scipy.stats as stats\nfrom IPython.display import display, HTML\n","metadata":{"execution":{"iopub.status.busy":"2023-04-29T01:00:17.521091Z","iopub.execute_input":"2023-04-29T01:00:17.521562Z","iopub.status.idle":"2023-04-29T01:00:17.529117Z","shell.execute_reply.started":"2023-04-29T01:00:17.521523Z","shell.execute_reply":"2023-04-29T01:00:17.527384Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Basic Data Exploration\n1. Import the data\n2. Look at summary statisitcs \n3. Evaluate Null Values\n\n#### Want more details on EDA? Check out this notebook: https://www.kaggle.com/code/kenjee/basic-eda-example-section-6","metadata":{}},{"cell_type":"code","source":"#import data \ndf = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")\n","metadata":{"execution":{"iopub.status.busy":"2023-04-29T01:00:20.429133Z","iopub.execute_input":"2023-04-29T01:00:20.429599Z","iopub.status.idle":"2023-04-29T01:00:20.460346Z","shell.execute_reply.started":"2023-04-29T01:00:20.429562Z","shell.execute_reply":"2023-04-29T01:00:20.45912Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to create scrollable table within a small window\ndef create_scrollable_table(df, table_id, title):\n    html = f'<h3>{title}</h3>'\n    html += f'<div id=\"{table_id}\" style=\"height:200px; overflow:auto;\">'\n    html += df.to_html()\n    html += '</div>'\n    return html","metadata":{"execution":{"iopub.status.busy":"2023-04-29T01:00:21.729784Z","iopub.execute_input":"2023-04-29T01:00:21.730241Z","iopub.status.idle":"2023-04-29T01:00:21.737321Z","shell.execute_reply.started":"2023-04-29T01:00:21.730198Z","shell.execute_reply":"2023-04-29T01:00:21.735939Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2023-04-29T01:02:44.564015Z","iopub.execute_input":"2023-04-29T01:02:44.564508Z","iopub.status.idle":"2023-04-29T01:02:44.574628Z","shell.execute_reply.started":"2023-04-29T01:02:44.564455Z","shell.execute_reply":"2023-04-29T01:02:44.572892Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"numerical_features = df.select_dtypes(include=[np.number])\nnumerical_features.describe()","metadata":{"execution":{"iopub.status.busy":"2023-04-29T01:00:40.190465Z","iopub.execute_input":"2023-04-29T01:00:40.190959Z","iopub.status.idle":"2023-04-29T01:00:40.327834Z","shell.execute_reply.started":"2023-04-29T01:00:40.19092Z","shell.execute_reply":"2023-04-29T01:00:40.326417Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Summary statistics for numerical features\nnumerical_features = df.select_dtypes(include=[np.number])\nsummary_stats = numerical_features.describe().T\nhtml_numerical = create_scrollable_table(summary_stats, 'numerical_features', 'Summary statistics for numerical features')\n\ndisplay(HTML(html_numerical))\n","metadata":{"execution":{"iopub.status.busy":"2023-04-29T01:00:43.240877Z","iopub.execute_input":"2023-04-29T01:00:43.241364Z","iopub.status.idle":"2023-04-29T01:00:43.345537Z","shell.execute_reply.started":"2023-04-29T01:00:43.241322Z","shell.execute_reply":"2023-04-29T01:00:43.34414Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Summary statistics for categorical features\ncategorical_features = df.select_dtypes(include=[object])\ncat_summary_stats = categorical_features.describe().T\nhtml_categorical = create_scrollable_table(cat_summary_stats, 'categorical_features', 'Summary statistics for categorical features')\n\ndisplay(HTML(html_categorical ))\n","metadata":{"execution":{"iopub.status.busy":"2023-04-29T01:04:12.220508Z","iopub.execute_input":"2023-04-29T01:04:12.221196Z","iopub.status.idle":"2023-04-29T01:04:12.304634Z","shell.execute_reply.started":"2023-04-29T01:04:12.22114Z","shell.execute_reply":"2023-04-29T01:04:12.303379Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Null values in the dataset\nnull_values = df.isnull().sum()\nhtml_null_values = create_scrollable_table(null_values.to_frame(), 'null_values', 'Null values in the dataset')\n\n# Percentage of missing values for each feature\nmissing_percentage = (df.isnull().sum() / len(df)) * 100\nhtml_missing_percentage = create_scrollable_table(missing_percentage.to_frame(), 'missing_percentage', 'Percentage of missing values for each feature')\n\ndisplay(HTML(html_null_values + html_missing_percentage))","metadata":{"execution":{"iopub.status.busy":"2023-04-29T01:04:45.918373Z","iopub.execute_input":"2023-04-29T01:04:45.919491Z","iopub.status.idle":"2023-04-29T01:04:45.946852Z","shell.execute_reply.started":"2023-04-29T01:04:45.919441Z","shell.execute_reply":"2023-04-29T01:04:45.945594Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Exploring rows with missing values\nrows_with_missing_values = df[df.isnull().any(axis=1)]\nhtml_rows_with_missing_values = create_scrollable_table(rows_with_missing_values.head(), 'rows_with_missing_values', 'Rows with missing values')\n\ndisplay(HTML(html_rows_with_missing_values))\n","metadata":{"execution":{"iopub.status.busy":"2023-04-28T23:11:03.265894Z","iopub.execute_input":"2023-04-28T23:11:03.267517Z","iopub.status.idle":"2023-04-28T23:11:03.31017Z","shell.execute_reply.started":"2023-04-28T23:11:03.267444Z","shell.execute_reply":"2023-04-28T23:11:03.30853Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2023-04-28T02:24:51.769188Z","iopub.execute_input":"2023-04-28T02:24:51.769526Z","iopub.status.idle":"2023-04-28T02:24:51.778583Z","shell.execute_reply.started":"2023-04-28T02:24:51.769494Z","shell.execute_reply":"2023-04-28T02:24:51.776999Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Explore the dependent variable \n- Should it be normalized? \n- Normalize Dependent Vairable","metadata":{}},{"cell_type":"code","source":"import scipy.stats as stats\n\n# Fit a normal distribution to the SalePrice data\nmu, sigma = stats.norm.fit(df['SalePrice'])\n\n# Create a histogram of the SalePrice column\nhist_data = go.Histogram(x=df['SalePrice'], nbinsx=50, name=\"Histogram\", opacity=0.75, histnorm='probability density', marker=dict(color='purple'))\n\n# Calculate the normal distribution based on the fitted parameters\nx_norm = np.linspace(df['SalePrice'].min(), df['SalePrice'].max(), 100)\ny_norm = stats.norm.pdf(x_norm, mu, sigma)\n\n# Create the normal distribution overlay\nnorm_data = go.Scatter(x=x_norm, y=y_norm, mode=\"lines\", name=f\"Normal dist. (μ={mu:.2f}, σ={sigma:.2f})\", line=dict(color=\"green\"))\n\n# Combine the histogram and the overlay\nfig = go.Figure(data=[hist_data, norm_data])\n\n# Set the layout for the plot\nfig.update_layout(\n    title=\"SalePrice Distribution\",\n    xaxis_title=\"SalePrice\",\n    yaxis_title=\"Density\",\n    legend_title_text=\"Fitted Normal Distribution\",\n    plot_bgcolor='rgba(32, 32, 32, 1)',\n    paper_bgcolor='rgba(32, 32, 32, 1)',\n    font=dict(color='white')\n)\n\n# Create a Q-Q plot\nqq_data = stats.probplot(df['SalePrice'], dist=\"norm\")\nqq_fig = px.scatter(x=qq_data[0][0], y=qq_data[0][1], labels={'x': 'Theoretical Quantiles', 'y': 'Ordered Values'}, color_discrete_sequence=[\"purple\"])\nqq_fig.update_layout(\n    title=\"Q-Q plot\",\n    plot_bgcolor='rgba(32, 32, 32, 1)',\n    paper_bgcolor='rgba(32, 32, 32, 1)',\n    font=dict(color='white')\n)\n\n# Calculate the line of best fit\nslope, intercept, r_value, p_value, std_err = stats.linregress(qq_data[0][0], qq_data[0][1])\nline_x = np.array(qq_data[0][0])\nline_y = intercept + slope * line_x\n\n# Add the line of best fit to the Q-Q plot\nline_data = go.Scatter(x=line_x, y=line_y, mode=\"lines\", name=\"Normal Line\", line=dict(color=\"green\"))\n\n# Update the Q-Q plot with the normal line\nqq_fig.add_trace(line_data)\n\n# Show the plots\nfig.show()\nqq_fig.show()\n\n#notebook credit: https://www.kaggle.com/code/serigne/stacked-regressions-top-4-on-leaderboard","metadata":{"execution":{"iopub.status.busy":"2023-04-28T02:24:51.780236Z","iopub.execute_input":"2023-04-28T02:24:51.780686Z","iopub.status.idle":"2023-04-28T02:24:53.74849Z","shell.execute_reply.started":"2023-04-28T02:24:51.78065Z","shell.execute_reply":"2023-04-28T02:24:53.747138Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# What questions do we want to ask of the data?\n\n1. Distribution of dwelling types and their relation to sale prices?\n2. Does zoning impact sale price?\n3. Does street and alley access types effect on sale price?\n4. What is the Average sale price by property shape?\n5. Is there a Correlation between Property Age and Sale Price\n6. Is there a Correlation between Living Area and Sale Price\n7. Does price change year to year?","metadata":{}},{"cell_type":"code","source":"# 1. Distribution of dwelling types and their relation to sale prices\ndwelling_types = df['BldgType'].value_counts()\ndwelling_prices = df.groupby('BldgType')['SalePrice'].mean()\n\n# Format labels for the second graph\nformatted_dwelling_prices = ['$' + f'{value:,.2f}' for value in dwelling_prices.values]\n\n# Create bar charts\nfig1 = go.Figure(data=[go.Bar(\n    x=dwelling_types.index,\n    y=dwelling_types.values,\n    marker_color='rgb(76, 175, 80)',\n    text=dwelling_types.values,\n    textposition='outside',\n    width=0.4,\n    marker=dict(line=dict(width=2, color='rgba(0,0,0,1)'), opacity=1)\n)])\nfig1.update_layout(\n    title='Distribution of Building Types',\n    xaxis_title='Building Type',\n    yaxis_title='Count',\n    plot_bgcolor='rgba(34, 34, 34, 1)',\n    paper_bgcolor='rgba(34, 34, 34, 1)',\n    font=dict(color='white')\n)\n\nfig2 = go.Figure(data=[go.Bar(\n    x=dwelling_prices.index,\n    y=dwelling_prices.values,\n    marker_color='rgb(156, 39, 176)',\n    text=formatted_dwelling_prices,\n    textposition='outside',\n    width=0.4,\n    marker=dict(line=dict(width=2, color='rgba(0,0,0,1)'), opacity=1)\n)])\nfig2.update_layout(\n    title='Average Sale Price by Building Type',\n    xaxis_title='Building Type',\n    yaxis_title='Price',\n    plot_bgcolor='rgba(34, 34, 34, 1)',\n    paper_bgcolor='rgba(34, 34, 34, 1)',\n    font=dict(color='white')\n)\n\n# Show the figures\nfig1.show()\nfig2.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-28T02:24:53.754831Z","iopub.execute_input":"2023-04-28T02:24:53.755337Z","iopub.status.idle":"2023-04-28T02:24:53.810022Z","shell.execute_reply.started":"2023-04-28T02:24:53.755293Z","shell.execute_reply":"2023-04-28T02:24:53.808866Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2. Zoning impact on sale price\nzoning_prices = df.groupby('MSZoning')['SalePrice'].mean()\nfig3 = px.bar(x=zoning_prices.index, y=zoning_prices.values, title='Average Sale Price by Zoning',\n              color_discrete_sequence=['purple', 'green'], text=zoning_prices.values,\n              template='plotly_dark')\n\nfig3.update_traces(texttemplate='$%{text:,.0f}', textposition='outside')\nfig3.update_yaxes(title='Sale Price', tickprefix='$', tickformat=',')\nfig3.update_xaxes(title='Zoning')\nfig3.update_layout(uniformtext_minsize=8, uniformtext_mode='hide')\n\nfig3.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-28T02:24:53.811785Z","iopub.execute_input":"2023-04-28T02:24:53.812516Z","iopub.status.idle":"2023-04-28T02:24:53.944144Z","shell.execute_reply.started":"2023-04-28T02:24:53.812472Z","shell.execute_reply":"2023-04-28T02:24:53.942731Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3. Street and alley access types effect on sale price\nstreet_prices = df.groupby('Street')['SalePrice'].mean()\nalley_prices = df.groupby('Alley')['SalePrice'].mean()\n\n# Street Prices\ncolors_street = np.where(street_prices.index == 'Pave', 'purple', 'green')\nfig5 = px.bar(x=street_prices.index, y=street_prices.values, title='Average Sale Price by Street Type',\n              template='plotly_dark', text=street_prices.values,\n              color=colors_street, color_discrete_sequence=['purple', 'green'])\n\nfig5.update_traces(texttemplate='$%{text:,.0f}', textposition='outside')\nfig5.update_yaxes(title='Sale Price', tickprefix='$', tickformat=',')\nfig5.update_xaxes(title='Street Type')\nfig5.update_layout(showlegend=False)\n\n# Alley Prices\ncolors_alley = np.where(alley_prices.index == 'Pave', 'purple', 'green')\nfig6 = px.bar(x=alley_prices.index, y=alley_prices.values, title='Average Sale Price by Alley Type',\n              template='plotly_dark', text=alley_prices.values,\n              color=colors_alley, color_discrete_sequence=['purple', 'green'])\n\nfig6.update_traces(texttemplate='$%{text:,.0f}', textposition='outside')\nfig6.update_yaxes(title='Sale Price', tickprefix='$', tickformat=',')\nfig6.update_xaxes(title='Alley Type')\nfig6.update_layout(showlegend=False)\n\nfig5.show()\nfig6.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-28T02:24:53.94564Z","iopub.execute_input":"2023-04-28T02:24:53.945982Z","iopub.status.idle":"2023-04-28T02:24:54.099163Z","shell.execute_reply.started":"2023-04-28T02:24:53.945939Z","shell.execute_reply":"2023-04-28T02:24:54.097751Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Reg: Regular\nIR1: Slightly irregular\nIR2: Moderately irregular\nIR3: Irregular","metadata":{}},{"cell_type":"code","source":"# 4. Average sale price by property shape\ncolors = px.colors.qualitative.Plotly\n\nshape_prices = df.groupby('LotShape')['SalePrice'].mean()\ncontour_prices = df.groupby('LandContour')['SalePrice'].mean()\n# Shape Prices\nfig7 = px.bar(x=shape_prices.index, y=shape_prices.values, title='Average Sale Price by Property Shape',\n              template='plotly_dark', text=shape_prices.values)\n\nfig7.update_traces(marker_color=colors, texttemplate='$%{text:,.0f}', textposition='outside')\nfig7.update_yaxes(title='Sale Price', tickprefix='$', tickformat=',')\nfig7.update_xaxes(title='Property Shape')\nfig7.update_layout(showlegend=False)\n\n# Contour Prices\nfig8 = px.bar(x=contour_prices.index, y=contour_prices.values, title='Average Sale Price by Property Contour',\n              template='plotly_dark', text=contour_prices.values)\n\nfig8.update_traces(marker_color=colors, texttemplate='$%{text:,.0f}', textposition='outside')\nfig8.update_yaxes(title='Sale Price', tickprefix='$', tickformat=',')\nfig8.update_xaxes(title='Property Contour')\nfig8.update_layout(showlegend=False)\n\nfig7.show()\nfig8.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-28T02:24:54.101049Z","iopub.execute_input":"2023-04-28T02:24:54.101886Z","iopub.status.idle":"2023-04-28T02:24:54.240216Z","shell.execute_reply.started":"2023-04-28T02:24:54.101825Z","shell.execute_reply":"2023-04-28T02:24:54.23882Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 5. Calculate Property Age\ndf['PropertyAge'] = df['YrSold'] - df['YearBuilt']\n\n# Calculate Correlation between Property Age and Sale Price\nage_price_corr = df['PropertyAge'].corr(df['SalePrice'])\nprint(f'Correlation between Property Age and Sale Price: {age_price_corr}')\n\n# Create a scatter plot to visualize the relationship between Property Age and Sale Price\nfig9 = px.scatter(df, x='PropertyAge', y='SalePrice', title='Property Age vs Sale Price', color='PropertyAge', color_continuous_scale=px.colors.sequential.Purp)\n\nfig9.update_layout(plot_bgcolor='rgb(30,30,30)', paper_bgcolor='rgb(30,30,30)', font=dict(color='white'))\n\nfig9.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-28T02:24:54.242237Z","iopub.execute_input":"2023-04-28T02:24:54.243093Z","iopub.status.idle":"2023-04-28T02:24:54.342383Z","shell.execute_reply.started":"2023-04-28T02:24:54.243023Z","shell.execute_reply":"2023-04-28T02:24:54.341106Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 6. Calculate Correlation between Living Area and Sale Price\nliving_area_price_corr = df['GrLivArea'].corr(df['SalePrice'])\nprint(f'Correlation between Living Area (above grade) and Sale Price: {living_area_price_corr}')\n\n# Create a scatter plot to visualize the relationship between Living Area and Sale Price\nfig10 = px.scatter(df, x='GrLivArea', y='SalePrice', title='Living Area (above grade) vs Sale Price', color='GrLivArea', color_continuous_scale=px.colors.sequential.Purp)\n\nfig10.update_layout(plot_bgcolor='rgb(30,30,30)', paper_bgcolor='rgb(30,30,30)', font=dict(color='white'))\n\nfig10.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-28T02:24:54.344209Z","iopub.execute_input":"2023-04-28T02:24:54.344686Z","iopub.status.idle":"2023-04-28T02:24:54.419125Z","shell.execute_reply.started":"2023-04-28T02:24:54.344636Z","shell.execute_reply":"2023-04-28T02:24:54.418094Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 7. Box plot of price over the years\nyearly_avg_sale_price = df.groupby('YrSold')['SalePrice'].mean()\n\nfig13 = px.box(df, x='YrSold', y='SalePrice', title='Sale Price Trends Over the Years',\n               points=False, color_discrete_sequence=['green'])\n\nfig13.add_trace(px.line(x=yearly_avg_sale_price.index, y=yearly_avg_sale_price.values).data[0])\n\nfig13.update_traces(line=dict(color='purple', width=4), selector=dict(type='scatter', mode='lines'))\n\nfor year, avg_price in yearly_avg_sale_price.items():\n    fig13.add_annotation(\n        x=year,\n        y=avg_price,\n        text=f\"{avg_price:,.0f}\",\n        font=dict(color='white'),\n        showarrow=False,\n        bgcolor='rgba(128, 0, 128, 0.6)'\n    )\n\nfig13.update_layout(\n    plot_bgcolor='rgb(30,30,30)',\n    paper_bgcolor='rgb(30,30,30)',\n    font=dict(color='white'),\n    xaxis_title='Year Sold',\n    yaxis_title='Sale Price'\n)\n\nfig13.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-28T02:24:54.420052Z","iopub.execute_input":"2023-04-28T02:24:54.420433Z","iopub.status.idle":"2023-04-28T02:24:54.607471Z","shell.execute_reply.started":"2023-04-28T02:24:54.4204Z","shell.execute_reply":"2023-04-28T02:24:54.606157Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Creating a Data Pipeline\nWhy do this? - So we have consistent infrastructure for transforming the test set\n\nGoal - To create infrastructure that lets us make changes without breaking everything ","metadata":{}},{"cell_type":"code","source":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\n\n# Define transformers for numerical and categorical columns\nnumerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse = False))\n])\n","metadata":{"execution":{"iopub.status.busy":"2023-04-28T02:24:54.609295Z","iopub.execute_input":"2023-04-28T02:24:54.610444Z","iopub.status.idle":"2023-04-28T02:24:55.011447Z","shell.execute_reply.started":"2023-04-28T02:24:54.610392Z","shell.execute_reply":"2023-04-28T02:24:55.010126Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Update categorical and numerical columns\ncategorical_columns = df.select_dtypes(include=['object', 'category']).columns\nnumerical_columns = df.select_dtypes(include=['int64', 'float64']).columns\n\n# Remove target variable from numerical columns\nnumerical_columns = numerical_columns.drop('SalePrice')\n\n# Combine transformers using ColumnTransformer\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_columns),\n        ('cat', categorical_transformer, categorical_columns)\n    ],remainder = 'passthrough')\n\n# Create a pipeline with the preprocessor\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor)])\n\n# Apply the pipeline to your dataset\nX = df.drop('SalePrice', axis=1)\ny = np.log(df['SalePrice']) #normalize dependent variable \nX_preprocessed = pipeline.fit_transform(X)","metadata":{"execution":{"iopub.status.busy":"2023-04-28T02:24:55.012964Z","iopub.execute_input":"2023-04-28T02:24:55.013391Z","iopub.status.idle":"2023-04-28T02:24:55.082886Z","shell.execute_reply.started":"2023-04-28T02:24:55.013351Z","shell.execute_reply":"2023-04-28T02:24:55.081534Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Fit and Parameter Tune models \n- We explore some different types of models here and see how they work (or don't work)\n\n#### Want more details on Parameter Tuning? Check out this notebook: https://www.kaggle.com/code/kenjee/model-building-example-section-11\n#### Want more details on Regression Models? Check out this notebook: https://www.kaggle.com/code/kenjee/model-evaluation-regression-12","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import GridSearchCV, KFold, cross_val_score\n\n# Split the data into training and testing sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)\n\n# Define the models\nmodels = {\n    'LinearRegression': LinearRegression(),\n    'RandomForest': RandomForestRegressor(random_state=42),\n    'XGBoost': XGBRegressor(random_state=42)\n}\n\n# Define the hyperparameter grids for each model\nparam_grids = {\n    'LinearRegression': {},\n    'RandomForest': {\n        'n_estimators': [100, 200, 500],\n        'max_depth': [None, 10, 30],\n        'min_samples_split': [2, 5, 10],\n    },\n    'XGBoost': {\n        'n_estimators': [100, 200, 500],\n        'learning_rate': [0.01, 0.1, 0.3],\n        'max_depth': [3, 6, 10],\n    }\n}\n\n# 3-fold cross-validation\ncv = KFold(n_splits=3, shuffle=True, random_state=42)\n\n# Train and tune the models\ngrids = {}\nfor model_name, model in models.items():\n    #print(f'Training and tuning {model_name}...')\n    grids[model_name] = GridSearchCV(estimator=model, param_grid=param_grids[model_name], cv=cv, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)\n    grids[model_name].fit(X_train, y_train)\n    best_params = grids[model_name].best_params_\n    best_score = np.sqrt(-1 * grids[model_name].best_score_)\n    \n    print(f'Best parameters for {model_name}: {best_params}')\n    print(f'Best RMSE for {model_name}: {best_score}\\n')","metadata":{"execution":{"iopub.status.busy":"2023-04-28T02:24:55.084688Z","iopub.execute_input":"2023-04-28T02:24:55.085101Z","iopub.status.idle":"2023-04-28T02:29:04.046631Z","shell.execute_reply.started":"2023-04-28T02:24:55.085038Z","shell.execute_reply":"2023-04-28T02:29:04.045176Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfrom sklearn.neural_network import MLPRegressor\n\nX_train_scaled = X_train.copy()\nX_test_scaled = X_test.copy()\n\n# Create an MLPRegressor instance\nmlp = MLPRegressor(random_state=42,max_iter=10000, n_iter_no_change = 3,learning_rate_init=0.001)\n\n# Define the parameter grid for tuning\nparam_grid = {\n    'hidden_layer_sizes': [(10,), (10,10), (10,10,10), (25)],\n    'activation': ['relu', 'tanh'],\n    'solver': ['adam'],\n    'alpha': [0.0001, 0.001, 0.01],\n    'learning_rate': ['constant', 'invscaling', 'adaptive'],\n}\n\n# Create the GridSearchCV object\ngrid_search_mlp = GridSearchCV(mlp, param_grid, scoring='neg_mean_squared_error', cv=3, n_jobs=-1, verbose=1)\n\n# Fit the model on the training data\ngrid_search_mlp.fit(X_train_scaled, y_train)\n\n# Print the best parameters found during the search\nprint(\"Best parameters found: \", grid_search_mlp.best_params_)\n\n# Evaluate the model on the test data\nbest_score = np.sqrt(-1 * grid_search_mlp.best_score_)\nprint(\"Test score: \", best_score)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-28T02:29:04.049449Z","iopub.execute_input":"2023-04-28T02:29:04.050179Z","iopub.status.idle":"2023-04-28T02:30:40.074186Z","shell.execute_reply.started":"2023-04-28T02:29:04.050126Z","shell.execute_reply":"2023-04-28T02:30:40.072762Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Principal Component Analysis \n- Basic Feature Engineering ","metadata":{}},{"cell_type":"code","source":"#pca\nfrom sklearn.decomposition import PCA\n\npca = PCA()\nX_pca_pre = pca.fit_transform(X_preprocessed)\n\n# Calculate the cumulative explained variance\ncumulative_explained_variance = np.cumsum(pca.explained_variance_ratio_)\n\n# Choose the number of components based on the explained variance threshold\nn_components = np.argmax(cumulative_explained_variance >= 0.95) + 1\n\npca = PCA(n_components=n_components)\npipeline_pca = Pipeline(steps=\n                        [('preprocessor', preprocessor),\n                        ('pca', pca)])\n\nX_pca = pipeline_pca.fit_transform(X)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-28T02:30:40.076278Z","iopub.execute_input":"2023-04-28T02:30:40.07718Z","iopub.status.idle":"2023-04-28T02:30:40.388952Z","shell.execute_reply.started":"2023-04-28T02:30:40.077125Z","shell.execute_reply":"2023-04-28T02:30:40.387076Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Running the same models with new data","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import GridSearchCV, KFold, cross_val_score\n\n# Split the data into training and testing sets\nfrom sklearn.model_selection import train_test_split\nX_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n\n# Define the models\nmodels = {\n    'LinearRegression': LinearRegression(),\n    'RandomForest': RandomForestRegressor(random_state=42),\n    'XGBoost': XGBRegressor(random_state=42)\n}\n\n# Define the hyperparameter grids for each model\nparam_grids = {\n    'LinearRegression': {},\n    'RandomForest': {\n        'n_estimators': [100, 200, 500],\n        'max_depth': [None, 10, 30],\n        'min_samples_split': [2, 5, 10],\n    },\n    'XGBoost': {\n        'n_estimators': [100, 200, 500],\n        'learning_rate': [0.01, 0.1, 0.3],\n        'max_depth': [3, 6, 10],\n    }\n}\n\n# 3-fold cross-validation\ncv = KFold(n_splits=3, shuffle=True, random_state=42)\n\n# Train and tune the models\ngrids_pca = {}\nfor model_name, model in models.items():\n    #print(f'Training and tuning {model_name}...')\n    grids_pca[model_name] = GridSearchCV(estimator=model, param_grid=param_grids[model_name], cv=cv, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)\n    grids_pca[model_name].fit(X_train_pca, y_train_pca)\n    best_params = grids_pca[model_name].best_params_\n    best_score = np.sqrt(-1 * grids_pca[model_name].best_score_)\n    \n    print(f'Best parameters for {model_name}: {best_params}')\n    print(f'Best RMSE for {model_name}: {best_score}\\n')","metadata":{"execution":{"iopub.status.busy":"2023-04-28T02:30:40.391511Z","iopub.execute_input":"2023-04-28T02:30:40.392526Z","iopub.status.idle":"2023-04-28T02:36:39.665079Z","shell.execute_reply.started":"2023-04-28T02:30:40.392468Z","shell.execute_reply":"2023-04-28T02:36:39.663938Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.neural_network import MLPRegressor\n\nX_train_scaled_pca = X_train_pca.copy()\nX_test_scaled_pca = X_test_pca.copy()\n\n# Create an MLPRegressor instance\nmlp = MLPRegressor(random_state=42,max_iter=10000, n_iter_no_change = 3,learning_rate_init=0.001)\n\n# Define the parameter grid for tuning\nparam_grid = {\n    'hidden_layer_sizes': [(10,), (10,10), (10,10,10), (25)],\n    'activation': ['relu', 'tanh'],\n    'solver': ['adam'],\n    'alpha': [0.0001, 0.001, 0.01, .1, 1],\n    'learning_rate': ['constant', 'invscaling', 'adaptive'],\n}\n\n# Create the GridSearchCV object\ngrid_search_mlp_pca = GridSearchCV(mlp, param_grid, scoring='neg_mean_squared_error', cv=3, n_jobs=-1, verbose=1)\n\n# Fit the model on the training data\ngrid_search_mlp_pca.fit(X_train_scaled_pca, y_train)\n\n# Print the best parameters found during the search\nprint(\"Best parameters found: \", grid_search_mlp_pca.best_params_)\n\n# Evaluate the model on the test data\nbest_score = np.sqrt(-1 * grid_search_mlp_pca.best_score_)\nprint(\"Test score: \", best_score)","metadata":{"execution":{"iopub.status.busy":"2023-04-28T02:36:39.670749Z","iopub.execute_input":"2023-04-28T02:36:39.672358Z","iopub.status.idle":"2023-04-28T02:42:46.291118Z","shell.execute_reply.started":"2023-04-28T02:36:39.672314Z","shell.execute_reply":"2023-04-28T02:42:46.289688Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfor i in grids.keys():\n    print (i + ': ' + str(np.sqrt(mean_squared_error(grids[i].predict(X_test), y_test))))","metadata":{"execution":{"iopub.status.busy":"2023-04-28T02:42:46.293307Z","iopub.execute_input":"2023-04-28T02:42:46.29376Z","iopub.status.idle":"2023-04-28T02:42:46.45881Z","shell.execute_reply.started":"2023-04-28T02:42:46.293722Z","shell.execute_reply":"2023-04-28T02:42:46.457856Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfor i in grids.keys():\n    print (i + ': ' + str(np.sqrt(mean_squared_error(grids_pca[i].predict(X_test_pca), y_test))))","metadata":{"execution":{"iopub.status.busy":"2023-04-28T02:42:46.461992Z","iopub.execute_input":"2023-04-28T02:42:46.463722Z","iopub.status.idle":"2023-04-28T02:42:46.62802Z","shell.execute_reply.started":"2023-04-28T02:42:46.46368Z","shell.execute_reply":"2023-04-28T02:42:46.627036Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print( str(np.sqrt(mean_squared_error(grid_search_mlp.predict(X_test_scaled),y_test))))","metadata":{"execution":{"iopub.status.busy":"2023-04-28T02:42:46.635621Z","iopub.execute_input":"2023-04-28T02:42:46.636611Z","iopub.status.idle":"2023-04-28T02:42:46.658894Z","shell.execute_reply.started":"2023-04-28T02:42:46.636564Z","shell.execute_reply":"2023-04-28T02:42:46.656693Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print( str(np.sqrt(mean_squared_error(grid_search_mlp_pca.predict(X_test_scaled_pca),y_test))))","metadata":{"execution":{"iopub.status.busy":"2023-04-28T02:42:46.661399Z","iopub.execute_input":"2023-04-28T02:42:46.662078Z","iopub.status.idle":"2023-04-28T02:42:46.67757Z","shell.execute_reply.started":"2023-04-28T02:42:46.662Z","shell.execute_reply":"2023-04-28T02:42:46.675839Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"var_explore = df[['Fence','Alley','MiscFeature','PoolQC','FireplaceQu','GarageCond','GarageQual','GarageFinish','GarageType','BsmtExposure','BsmtFinType2','BsmtFinType1','BsmtCond','BsmtQual','MasVnrType','Electrical','MSZoning','Utilities','Exterior1st','Exterior2nd','KitchenQual','Functional','SaleType','LotFrontage','GarageYrBlt','MasVnrArea','BsmtFullBath','BsmtHalfBath','GarageCars','GarageArea','TotalBsmtSF']]\n\ndisplay(HTML(create_scrollable_table(var_explore, 'var_explore', 'List of Variables to Explore for Feature Engineering')))","metadata":{"execution":{"iopub.status.busy":"2023-04-28T02:42:46.679791Z","iopub.execute_input":"2023-04-28T02:42:46.681369Z","iopub.status.idle":"2023-04-28T02:42:47.452845Z","shell.execute_reply.started":"2023-04-28T02:42:46.681303Z","shell.execute_reply":"2023-04-28T02:42:47.451599Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import FunctionTransformer\n\n# feature engineering functions \ndef custom_features(df):\n    df_out = df.copy()\n    df_out['PropertyAge'] = df_out['YrSold'] - df_out['YearBuilt']\n    df_out['TotalSF'] = df_out['TotalBsmtSF'] + df_out['1stFlrSF'] + df_out['2ndFlrSF']\n    df_out['TotalBath'] = df_out['FullBath'] + 0.5 * df_out['HalfBath'] + df_out['BsmtFullBath'] + 0.5 * df['BsmtHalfBath']\n    df_out['HasRemodeled'] = (df_out['YearRemodAdd'] != df_out['YearBuilt']).astype(object)\n    df_out['Has2ndFloor'] = (df_out['2ndFlrSF'] > 0).astype(object)\n    df_out['HasGarage'] = (df_out['GarageArea'] > 0).astype(object)\n    df_out['YrSold_cat'] = df_out['YrSold'].astype(object)\n    df_out['MoSold_cat'] = df_out['MoSold'].astype(object)\n    df_out['YearBuilt_cat'] = df_out['YearBuilt'].astype(object)\n    df_out['MSSubClass_cat'] = df_out['MSSubClass'].astype(object)\n    \n    return df_out\n\nfeature_engineering_transformer = FunctionTransformer(custom_features)\n\n\n#make this better, one functtion? get new variables in the pipeline? ","metadata":{"execution":{"iopub.status.busy":"2023-04-28T02:42:47.4543Z","iopub.execute_input":"2023-04-28T02:42:47.454649Z","iopub.status.idle":"2023-04-28T02:42:47.465311Z","shell.execute_reply.started":"2023-04-28T02:42:47.454617Z","shell.execute_reply":"2023-04-28T02:42:47.463806Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Identify categorical and numerical columns\nnew_cols_categorical = pd.Index(['HasRemodeled', 'Has2ndFloor', 'HasGarage'])\nnew_cols_numeric = pd.Index(['PropertyAge', 'TotalSF', 'TotalBath', 'YrSold_cat', 'MoSold_cat', 'YearBuilt_cat', 'MSSubClass_cat'])\n\n# Update categorical and numerical columns\ncategorical_columns = df.select_dtypes(include=['object', 'category']).columns.append(new_cols_categorical)\nnumerical_columns = df.select_dtypes(include=['int64', 'float64']).columns.append(new_cols_numeric)\n\n# Remove target variable from numerical columns\nnumerical_columns = numerical_columns.drop('SalePrice')\n\n# Combine transformers using ColumnTransformer\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_columns),\n        ('cat', categorical_transformer, categorical_columns)\n    ],remainder = 'passthrough')\n\n# Create a pipeline with the preprocessor\npipeline_fe = Pipeline(steps=[\n    ('fe', feature_engineering_transformer),\n    ('preprocessor', preprocessor),\n    ('pca', pca)])\n\n# Apply the pipeline to your dataset\nX = df.drop('SalePrice', axis=1)\ny = np.log(df['SalePrice'])\nX_preprocessed_fe = pipeline_fe.fit_transform(X)","metadata":{"execution":{"iopub.status.busy":"2023-04-28T02:42:47.467457Z","iopub.execute_input":"2023-04-28T02:42:47.467825Z","iopub.status.idle":"2023-04-28T02:42:47.633506Z","shell.execute_reply.started":"2023-04-28T02:42:47.467791Z","shell.execute_reply":"2023-04-28T02:42:47.631782Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split the data into training and testing sets\nfrom sklearn.model_selection import train_test_split\nX_train_fe, X_test_fe, y_train_fe, y_test_fe = train_test_split(X_preprocessed_fe, y, test_size=0.2, random_state=42)\n\n# Define the models\nmodels = {\n    'LinearRegression': LinearRegression(),\n    'RandomForest': RandomForestRegressor(random_state=42),\n    'XGBoost': XGBRegressor(random_state=42)\n}\n\n# Define the hyperparameter grids for each model\nparam_grids = {\n    'LinearRegression': {},\n    'RandomForest': {\n        'n_estimators': [100, 200, 500],\n        'max_depth': [None, 10, 30],\n        'min_samples_split': [2, 5, 10],\n    },\n    'XGBoost': {\n        'n_estimators': [100, 200, 500],\n        'learning_rate': [0.01, 0.1, 0.3],\n        'max_depth': [3, 6, 10],\n    }\n}\n\n# 3-fold cross-validation\ncv = KFold(n_splits=3, shuffle=True, random_state=42)\n\n# Train and tune the models\ngrids_fe = {}\nfor model_name, model in models.items():\n    #print(f'Training and tuning {model_name}...')\n    grids_fe[model_name] = GridSearchCV(estimator=model, param_grid=param_grids[model_name], cv=cv, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)\n    grids_fe[model_name].fit(X_train_fe, y_train_fe)\n    best_params = grids_fe[model_name].best_params_\n    best_score = np.sqrt(-1 * grids_fe[model_name].best_score_)\n    \n    print(f'Best parameters for {model_name}: {best_params}')\n    print(f'Best RMSE for {model_name}: {best_score}\\n')","metadata":{"execution":{"iopub.status.busy":"2023-04-28T02:42:47.635923Z","iopub.execute_input":"2023-04-28T02:42:47.637264Z","iopub.status.idle":"2023-04-28T02:48:47.923956Z","shell.execute_reply.started":"2023-04-28T02:42:47.637183Z","shell.execute_reply":"2023-04-28T02:48:47.922831Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_scaled_fe = X_train_fe.copy()\nX_test_scaled_fe = X_test_fe.copy()\n\n# Create an MLPRegressor instance\nfrom sklearn.neural_network import MLPRegressor\n\nmlp = MLPRegressor(random_state=42, max_iter=10000, n_iter_no_change=3)\n\n# Define the parameter grid for tuning\nparam_grid = {\n    'hidden_layer_sizes': [(10,), (10, 10), (10, 25)],\n    'activation': ['relu', 'tanh', 'sigmoid'],\n    'solver': ['adam', 'sgd'],\n    'alpha': [.1, .5, 1, 10, 100],\n    'learning_rate': ['constant', 'invscaling', 'adaptive'],\n    'learning_rate_init' : [0.1]\n}\n\n# Create the GridSearchCV object\nfrom sklearn.model_selection import GridSearchCV\ngrid_search_mlp_fe = GridSearchCV(mlp, param_grid, scoring='neg_mean_squared_error', cv=3, n_jobs=-1, verbose=1)\n\n# Fit the model on the training data\ngrid_search_mlp_fe.fit(X_train_scaled_fe, y_train_fe)\n\n# Print the best parameters found during the search\nprint(\"Best parameters found: \", grid_search_mlp_fe.best_params_)\n\n# Evaluate the model on the test data\nbest_score = np.sqrt(-1 * grid_search_mlp_fe.best_score_)\nprint(\"Test score: \", best_score)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-28T02:48:47.926944Z","iopub.execute_input":"2023-04-28T02:48:47.928Z","iopub.status.idle":"2023-04-28T02:57:03.600351Z","shell.execute_reply.started":"2023-04-28T02:48:47.927958Z","shell.execute_reply":"2023-04-28T02:57:03.598122Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfor i in grids.keys():\n    print (i + ': ' + str(np.sqrt(mean_squared_error(grids_fe[i].predict(X_test_fe), y_test))))","metadata":{"execution":{"iopub.status.busy":"2023-04-28T02:57:03.602754Z","iopub.execute_input":"2023-04-28T02:57:03.60331Z","iopub.status.idle":"2023-04-28T02:57:03.763499Z","shell.execute_reply.started":"2023-04-28T02:57:03.603257Z","shell.execute_reply":"2023-04-28T02:57:03.76251Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print( str(np.sqrt(mean_squared_error(grid_search_mlp_fe.predict(X_test_scaled_fe),y_test))))","metadata":{"execution":{"iopub.status.busy":"2023-04-28T02:57:03.766481Z","iopub.execute_input":"2023-04-28T02:57:03.767428Z","iopub.status.idle":"2023-04-28T02:57:03.774772Z","shell.execute_reply.started":"2023-04-28T02:57:03.767385Z","shell.execute_reply":"2023-04-28T02:57:03.773863Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')","metadata":{"execution":{"iopub.status.busy":"2023-04-28T02:57:03.775882Z","iopub.execute_input":"2023-04-28T02:57:03.776508Z","iopub.status.idle":"2023-04-28T02:57:03.82825Z","shell.execute_reply.started":"2023-04-28T02:57:03.776472Z","shell.execute_reply":"2023-04-28T02:57:03.826768Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test_preprocessed = pipeline_fe.transform(df_test)","metadata":{"execution":{"iopub.status.busy":"2023-04-28T02:57:03.830225Z","iopub.execute_input":"2023-04-28T02:57:03.831269Z","iopub.status.idle":"2023-04-28T02:57:03.909394Z","shell.execute_reply.started":"2023-04-28T02:57:03.831207Z","shell.execute_reply":"2023-04-28T02:57:03.907531Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#xgboost submission\ny_xgboost = np.exp(grids_fe['XGBoost'].predict(df_test_preprocessed))\n\ndf_xgboost_out = df_test[['Id']].copy()\ndf_xgboost_out['SalePrice'] = y_xgboost\n\n#\ndf_xgboost_out.to_csv('submission_xgboost_new_features_normalized.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-28T02:57:03.915264Z","iopub.execute_input":"2023-04-28T02:57:03.918586Z","iopub.status.idle":"2023-04-28T02:57:03.981542Z","shell.execute_reply.started":"2023-04-28T02:57:03.918498Z","shell.execute_reply":"2023-04-28T02:57:03.980448Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#rf submission\ny_rf = np.exp(grids_fe['RandomForest'].predict(df_test_preprocessed))\n\ndf_rf_out = df_test[['Id']].copy()\ndf_rf_out['SalePrice'] = y_rf\n\n#\ndf_rf_out.to_csv('submission_rf_normalized.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-28T02:57:03.985944Z","iopub.execute_input":"2023-04-28T02:57:03.989606Z","iopub.status.idle":"2023-04-28T02:57:04.198038Z","shell.execute_reply.started":"2023-04-28T02:57:03.989504Z","shell.execute_reply":"2023-04-28T02:57:04.196636Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#mlp submission\ny_mlp = np.exp(grid_search_mlp_fe.predict(df_test_preprocessed))\n\ndf_mlp_out = df_test[['Id']].copy()\ndf_mlp_out['SalePrice'] = y_mlp\n\ndf_mlp_out.to_csv('submission_mlp_normalized.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-28T02:57:04.199853Z","iopub.execute_input":"2023-04-28T02:57:04.200366Z","iopub.status.idle":"2023-04-28T02:57:04.231182Z","shell.execute_reply.started":"2023-04-28T02:57:04.200317Z","shell.execute_reply":"2023-04-28T02:57:04.229142Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_avg_ens = (y_rf + y_xgboost + y_mlp)/3\n\n#xgboost submission\ndf_avg_ens_out = df_test[['Id']].copy()\ndf_avg_ens_out['SalePrice'] = y_avg_ens\n\n#\ndf_avg_ens_out.to_csv('submission_avg_ens_new_features_normalized.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-28T02:57:04.233717Z","iopub.execute_input":"2023-04-28T02:57:04.234427Z","iopub.status.idle":"2023-04-28T02:57:04.258522Z","shell.execute_reply.started":"2023-04-28T02:57:04.234347Z","shell.execute_reply":"2023-04-28T02:57:04.256371Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.ensemble import StackingRegressor\n\ngrids_fe['MLP'] =   grid_search_mlp_fe\n\nbest_estimators = [(model_name, grid.best_estimator_) for model_name, grid in grids_fe.items()]\n\n# Define the candidate meta-models\nmeta_models = {\n    'MLP': MLPRegressor(random_state=42, max_iter=10000, n_iter_no_change=3, learning_rate_init=0.001),\n    'LinearRegression': LinearRegression(),\n    'XGBoost': XGBRegressor(random_state=42)\n}\n\n# Define the hyperparameter grids for each meta-model\nmeta_param_grids = {\n    'MLP': {\n        'final_estimator__hidden_layer_sizes': [(10,), (10, 10)],\n        'final_estimator__activation': ['relu', 'tanh'],\n        'final_estimator__solver': ['adam', 'sgd'],\n        'final_estimator__alpha': [ 0.001, 0.01, .1, .5],\n        'final_estimator__learning_rate': ['constant', 'invscaling', 'adaptive'],\n    },\n    'LinearRegression': {},\n    'XGBoost': {\n        'final_estimator__n_estimators': [100, 200, 500],\n        'final_estimator__learning_rate': [0.01, 0.1, 0.3],\n        'final_estimator__max_depth': [3, 6, 10],\n    }\n}\n\n# 3-fold cross-validation\ncv = KFold(n_splits=3, shuffle=True, random_state=42)\n\n# Train and tune the stacking ensemble\nbest_score = float('inf')\nbest_model = None\n\nfor meta_name, meta_model in meta_models.items():\n    print(f'Training and tuning {meta_name} as the meta-model...')\n    stacking_regressor = StackingRegressor(estimators=best_estimators, final_estimator=meta_model, cv=cv)\n    grid_search = GridSearchCV(estimator=stacking_regressor, param_grid=meta_param_grids[meta_name], cv=cv, scoring='neg_mean_squared_error', n_jobs=-1, verbose=1)\n    grid_search.fit(X_train_fe, y_train_fe)\n    best_params = grid_search.best_params_\n    best_rmse = np.sqrt(-1 * grid_search.best_score_)\n    \n    print(f'Best parameters for {meta_name}: {best_params}')\n    print(f'Best RMSE for {meta_name}: {best_rmse}\\n')\n    \n    if best_rmse < best_score:\n        best_score = best_rmse\n        best_model = grid_search\n\n# Evaluate the best stacking ensemble on the test data\ny_pred = best_model.predict(X_test_fe)\nrmse = np.sqrt(mean_squared_error(y_test_fe, y_pred))\nprint(f\"Best stacking ensemble's RMSE on test data: {rmse}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-28T02:57:04.261117Z","iopub.execute_input":"2023-04-28T02:57:04.262299Z","iopub.status.idle":"2023-04-28T05:40:21.018687Z","shell.execute_reply.started":"2023-04-28T02:57:04.262244Z","shell.execute_reply":"2023-04-28T05:40:21.017635Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"y_stack = np.exp(best_model.predict(df_test_preprocessed))\n\n#xgboost submission\n\ndf_stack_out = df_test[['Id']].copy()\ndf_stack_out['SalePrice'] = y_stack\n\ndf_stack_out.to_csv('submission_stack_new_features_normalized.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-28T06:53:48.248832Z","iopub.execute_input":"2023-04-28T06:53:48.249299Z","iopub.status.idle":"2023-04-28T06:53:48.545674Z","shell.execute_reply.started":"2023-04-28T06:53:48.249257Z","shell.execute_reply":"2023-04-28T06:53:48.544561Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_stack_out.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-28T06:55:53.302472Z","iopub.execute_input":"2023-04-28T06:55:53.30289Z","iopub.status.idle":"2023-04-28T06:55:53.314627Z","shell.execute_reply.started":"2023-04-28T06:55:53.302856Z","shell.execute_reply":"2023-04-28T06:55:53.313262Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Check out these additional Resources for continued learning! \n\n### Titanic Beginner Walkthrough \n- Kaggle notebook: https://www.kaggle.com/code/kenjee/titanic-project-example/notebook\n- YouTube Video: https://www.youtube.com/watch?v=I3FBJdiExcg&ab_channel=KenJee\n\n### My Other Notebooks for Learning the ML Process\n- [**Dealing with Missing Values - Section 5.1**](https://www.kaggle.com/code/kenjee/dealing-with-missing-values-section-5-1)\n- [**Dealing with Outliers - Section 5.2**](https://www.kaggle.com/code/kenjee/dealing-with-outliers-section-5-2)\n- [**Basic EDA Example - Section 6**](https://www.kaggle.com/code/kenjee/basic-eda-example-section-6)\n- [**Categorical Feature Engineering - Section 7.1**](https://www.kaggle.com/code/kenjee/categorical-feature-engineering-section-7-1)\n- [**Numeric Feature Engineering - Section 7.2**](https://www.kaggle.com/kenjee/numeric-feature-engineering-section-7-2)\n- [**Cross Validation Foundations - Section 8**](https://www.kaggle.com/code/kenjee/cross-validation-foundations-section-8)\n- [**Feature Selection - Section 9**](https://www.kaggle.com/code/kenjee/feature-selection-section-9)\n- [**Dealing with Imbalanced Data - Section 10**](https://www.kaggle.com/code/kenjee/dealing-with-imbalanced-data-section-10)\n- [**Model Building Example - Section 11**](https://www.kaggle.com/code/kenjee/model-building-example-section-11)\n- [**Model Evaluation (Classification) - Section 11**](https://www.kaggle.com/code/kenjee/model-evaluation-classification-section-12)\n- [**Model Evlauation (Regression) - Section 11**](https://www.kaggle.com/code/kenjee/model-evaluation-regression-12)\n\n### My Other Notebooks for Learning ML Algorithms\n- [**Exhaustive Regression with Parameter Tuning**](https://www.kaggle.com/code/kenjee/exhaustive-regression-parameter-tuning)\n- [**Exhaustive Classification with Parameter Tuning**](https://www.kaggle.com/code/kenjee/exhaustive-classification-parameter-tuning)\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}